{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6 - Voice Cloning and Fake Audio Detection (VCFAD)\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "A technology company working in the Cyber Security industry which focuses on building systems that help individuals and organizations to have safe and secure digital presence requires an algorithm that can synthesize spoken audio by converting a speaker’s voice to another speaker’s voice with the end goal to detect if any spoken audio is pristine or fake.\n",
    "\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "Two datasets will be used in this project:\n",
    "- **TIMIT Dataset:** The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains a total of 6300 sentences, 10 sentences spoken by each of 630 speakers from 8 major dialect regions of the United States.\n",
    "\n",
    "- **CommonVoice Dataset:** Common Voice is part of Mozilla's initiative to help teach machines how real people speak. Common Voice is a corpus of speech data read by users on the Common Voice website (https://commonvoice.mozilla.org/), and based upon text from a number of public domain sources like user submitted blog posts, old books, movies, and other public speech corpora. Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems.\n",
    "\n",
    "\n",
    "**Goal(s):**\n",
    "\n",
    "- Build a machine learning system to detect if a spoken audio is synthetically generated or not.\n",
    "\n",
    "    - First, build a voice cloning system (VC) given a speaker’s spoken audio that clones the source speaker’s voice to the target speaker’s voice. Utilize the TIMIT dataset as it consists of aligned text-audio data with various speakers.\n",
    "    \n",
    "    - Next, build a machine learning system which detects if any spoken audio is a natural speech or synthetically generated by machine. Utilize the CommonVoice dataset as it consists of thousands of naturally spoken audio which could be used as golden spoken audio by humans as positive examples and creating negative examples using the voice cloning system as automatic data/label generator. Since the CommonVoice English dataset is large, you can use a subset of it by sampling the dataset.\n",
    "\n",
    "\n",
    "**Success Metrics:**\n",
    "\n",
    "\n",
    "- Voice cloning (VC):\n",
    "    - use Word Error Rate (WER)\n",
    "    - report speaker classification accuracy\n",
    "<br><br>\n",
    "- Fake audio detection (FAD):\n",
    "    - Use F-score via positive labels coming from the groundtruth dataset and negative labels generated by the VC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice cloning system (VC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandrabinder/Projects/Apziva/P6_Voice_cloning_and_fake_audio_detection/Real_Time_Voice_Cloning/encoder/audio.py:13: UserWarning: Unable to import 'webrtcvad'. This package enables noise removal and is recommended.\n",
      "  warn(\"Unable to import 'webrtcvad'. This package enables noise removal and is recommended.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(\"Real_Time_Voice_Cloning\")\n",
    "\n",
    "from encoder import inference as encoder\n",
    "from synthesizer.inference import Synthesizer\n",
    "from Real_Time_Voice_Cloning.utils.default_models import ensure_default_models\n",
    "from vocoder import inference as vocoder\n",
    "\n",
    "import functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the voice cloning system we are going to use the TIMIT dataset which contains a total of 6300 sentences, 10 sentences spoken by each of 630\n",
    "speakers from 8 major dialect regions of the United States."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000: 01001110 01001001 01010011 01010100 01011111 00110001  NIST_1\n",
      "00000006: 01000001 00001010 00100000 00100000 00100000 00110001  A.   1\n",
      "0000000c: 00110000 00110010 00110100 00001010 01100100 01100001  024.da\n",
      "00000012: 01110100 01100001 01100010 01100001 01110011 01100101  tabase\n",
      "00000018: 01011111 01101001 01100100 00100000 00101101 01110011  _id -s\n",
      "0000001e: 00110101 00100000 01010100 01001001 01001101 01001001  5 TIMI\n",
      "00000024: 01010100 00001010 01100100 01100001 01110100 01100001  T.data\n",
      "0000002a: 01100010 01100001 01110011 01100101 01011111 01110110  base_v\n",
      "00000030: 01100101 01110010 01110011 01101001 01101111 01101110  ersion\n",
      "00000036: 00100000 00101101 01110011 00110011 00100000 00110001   -s3 1\n",
      "xxd: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!xxd -b ./data/TIMIT/TRAIN/DR1/FCJF0/SA1.WAV | head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since files start with a NIST_1 header instead of a RIFF header, we have to convert them to a proper .wav format to be able to use de sound_recognition functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_AUDIOFILES_PATH = \"data/TIMIT/TRAIN\"\n",
    "functions.convert_audiofiles(ORIGINAL_AUDIOFILES_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract .wav files from the original folder and organize them in a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ORIGINAL_WAV_PATH): os.makedirs(ORIGINAL_WAV_PATH)\n",
    "if not os.path.exists(FAKE_WAV_PATH): os.makedirs(FAKE_WAV_PATH)\n",
    "\n",
    "functions.organize_audio_files(root_path='data/TIMIT/TRAIN', new_folder=ORIGINAL_WAV_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice cloning process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the functions from the following github:\n",
    "`https://github.com/CorentinJ/Real-Time-Voice-Cloning.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs available. Using GPU 0 (NVIDIA GeForce RTX 3060 Laptop GPU) of compute capability 8.6 with 6.2Gb total memory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "functions.check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder \"encoder.pt\" trained to step 1564501\n",
      "Synthesizer using device: cuda\n",
      "Building Wave-RNN\n",
      "Trainable Parameters: 4.481M\n",
      "Loading model weights at saved_models/default/vocoder.pt\n"
     ]
    }
   ],
   "source": [
    "## Load the models one by one.\n",
    "ensure_default_models(Path(\"saved_models\"));\n",
    "encoder.load_model(Path(\"saved_models/default/encoder.pt\"));\n",
    "synthesizer = Synthesizer(\"saved_models/default/synthesizer.pt\");\n",
    "vocoder.load_model(Path(\"saved_models/default/vocoder.pt\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_voice_files = glob.glob(os.path.join(ORIGINAL_WAV_PATH, \"*\"))\n",
    "for i, path in enumerate(original_voice_files):\n",
    "    try:\n",
    "        src_audio = path\n",
    "        if i < len(original_voice_files)-1: sample_audio = original_voice_files[i+1]\n",
    "        else: sample_audio = original_voice_files[0] \n",
    "        dst_audio = os.path.join(FAKE_WAV_PATH, \"fake_\" + src_audio.split('/')[-1])\n",
    "        functions.voice_to_voice(src_audio, sample_audio, dst_audio)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Caught exception: %s\" % repr(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MISSING:\n",
    "- use Word Error Rate (WER)\n",
    "- report speaker classification accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Audio Detection (FAD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MISSING:\n",
    "- Use F-score via positive labels coming from the groundtruth dataset and negative labels generated by the VC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apziva-p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
